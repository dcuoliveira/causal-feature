{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of feature selection methodologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING** The feature selection procedures presented below have assumed that we are dealing with the specific problem of predicting a stock market index directional movements using a matrix of features, in particular we are using a matrix of data from the google trends search engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that $R_t$ is the return for a market index at time $t$, and is defined according to:\n",
    "\n",
    "$$\n",
    "R_t = \\frac{R_t}{R_{t-1}} - 1\n",
    "$$\n",
    "\n",
    "We define the directional movement associated with $R_t$ as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\delta_t=\n",
    "\\begin{cases}\n",
    "  1, & \\text{if}\\ R_t > R_{t-1} \\\\\n",
    "  0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    " $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Ex ante selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Standard methodology in finance\n",
    "\n",
    "* Select features according to economic, behavioral or finance theory\n",
    "\n",
    "* Papers that argument in favor of this feature selection procedure:\n",
    "    >* [Arnott et al. (2018)](https://jfds.pm-research.com/content/1/1/64)\n",
    "    \n",
    "* Papers that uses this feature selection procedure, in particular the ones that deals with google trends data:\n",
    "    >* [Preis et al. (2013)](https://www.nature.com/articles/srep01684)\n",
    "    >* [Curme et al. (2014)](https://www.pnas.org/content/111/32/11600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data driven selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Association search procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Johansen cointegration test\n",
    "* Applied in [Huang et al. (2019)](https://link.springer.com/article/10.1007/s00181-019-01725-1)\n",
    "* **Assumptions Johansen (1991):**\n",
    "> 1. large sample size\n",
    "> 2. all series must be integrated of order d\n",
    "\n",
    "\n",
    "> **Definition (stationarity):** A stochastic process $\\mathbf{r}_t$ is said to be strictly stationary if the joint distribution of $(r_{1t},...,r_{nt})$ is invariant under time shift. A process of the same type is said to be weakly stationary if both the mean of $\\mathbf{r}_t$ and the covariance between $\\mathbf{r}_t$ and $\\mathbf{r}_{t-l}$ are time-invariant, for $l \\in \\mathbb{Z}$.\n",
    "\n",
    "> **Definition (order of integration):** The order of integration denoted by $I(p)$ is a summary statistics which reports the number of differences required to obtain a covariance stationary series. In other words, a time series $r_it$, for $i=1,..,n$, is integrated of order p if\n",
    "$$\n",
    "(1-L)^dr_t\n",
    "$$\n",
    "is a stochastic process, where $L$ is the lag operator, and $(1-L)$ is the first difference.\n",
    "\n",
    "> **Definition (cointegrated series) [Enders (2014)](https://www.wiley.com/en-us/Applied+Econometric+Time+Series%2C+4th+Edition-p-9781118808566):** We say that the components of $\\mathbf{r}_t=(r_{1t},...,r_{nt})^{'}$ are cointegrated of order $d$, $b$, denoted $\\mathbf{r}_t \\sim CI(d,b)$ if\n",
    "1. All components of $\\mathbf{r}_t$ are integrated of order d\n",
    "2. There exists a vector $\\mathbf{\\beta}=(\\beta_1,...,\\beta_n)^{'}$ (cointegration vector) such that the linear combination $\\mathbf{\\beta}\\mathbf{r}_t=\\beta_1 r_{1t}+...+\\beta_n r_{nt}$ is integrated of order $(d-b)$, where $b>0$.\n",
    "\n",
    "> **Definition (vector autoregressive model) [Tsay (2010)](https://www.wiley.com/en-us/Analysis+of+Financial+Time+Series%2C+3rd+Edition-p-9780470414354):** Let $\\mathbf{r}_t$ be a stochastic process which represent the return of an asset. We say that $\\mathbf{r}_t$ follows a VAR(p) model if it satisfies\n",
    "$$\n",
    "\\mathbf{r}_t=\\phi_0 + \\Phi_1\\mathbf{r}_{t-1} + ... + \\Phi_p\\mathbf{r}_{t-p} + \\mathbf{a}_t, p>0\n",
    "$$\n",
    "where $\\phi_0$ is a k-dimensional vector, $\\mathbf{a}_t$ is a sequence of serially uncorrelated randon vectors with mean 0 and covariance matrix $\\Sigma$, and $\\Phi_j$ are $kxk$ matrices.\n",
    "\n",
    "> **Definition (Johansen VAR model) [Johansen (1991)](https://www.jstor.org/stable/2938278?seq=1):** * Let $\\mathbf{r}_t$ be a stochastic process which represent the return of an asset. We say that $\\mathbf{r}_t$ follows a VAR(p) Johansem model if it satisfies\n",
    "$$\n",
    "\\mathbf{r}_t = \\sum_{i=1}^{k-1}\\Phi_i\\mathbf{r}_{t-i} + \\Gamma\\mathbf{r}_{t-k} + \\Pi\\mathbf{d}_t + \\mu + \\mathbf{a}_t\n",
    "$$\n",
    "where the first term is similar to the first definition, $\\Gamma$ is the matrix where we formulate the hypothesis of interest as restrictions in the matrix, and $\\mathbf{d}_t$ are seasonal dummies. The authors formulates the hypothesis in the following way:\n",
    "<!-- $$\n",
    "\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Mean drecrease impurity\n",
    "* In-sample method, specific to tree-based classifiers\n",
    "* The intuition behind the method is to infer the importance of the features based on how much of the \"impurity\" of the sample is eliminated by including the feature in the model\n",
    "\n",
    "> **Definition (binary classification tree) [Louppe et al. (2014)](https://papers.nips.cc/paper/4928-understanding-variable-importances-in-forests-of-randomized-trees):** A binary classification tree is an input-output model represented by a tree structure $T$ from a random input vector $(X_1,...,X_p)$ taking its values in $\\mathcal{X}_1\\times...\\times\\mathcal{X}_p=\\mathcal{X}$ to a random output $Y \\in \\mathcal{Y}$.\n",
    "\n",
    "* Any node $t$ of $T$ represents a subset of $\\mathcal{X}$, and are labeled with an internal test of the type $S_t=(X_m<l)$ which divides the subset into two nodes $t_l$ and $t_r$.\n",
    "* A tree is built from an $N$ sample size of $\\mathbf{P}(Y,X_1,...,X_p)$ using a recursive procedure that identifies at each node $t$ the split $S_t=S^{*}_t$ for which the partition of the $N_t$ node samples into $t_l$ and $t_r$ maximizes the decrease in\n",
    "$$\n",
    "\\delta_i(s,t)=i(t)-p_li(t_l)-p_ri(t_r)\n",
    "$$\n",
    "\n",
    "where $i(t)$ is some impurity measure, and $p_l=N_{tl}/N_t$ and $p_r=N_{tr}/N_t$ are the proportion of node samples to the left and right relative to the total number of node samples up to $t$ respectivly.\n",
    "\n",
    "> **Definition (mean decrease impurity) [Breiman (2001)](https://link.springer.com/article/10.1023/A:1010933404324):** Lets define the impurity decrease as the equation above, $p(t)=N_t/N$ as the proportion of node samples until $t$ relative to the total number of nodes, and $v(S_t)$ as the variable used in split $S_T$. Then, the mean decrease impurity for random variable $X_m$ is defined as\n",
    "$$\n",
    "I(X_m)=\\frac{1}{N_T}\\sum_{T}^{}\\sum_{t \\in T: v(s_t)=X_m}^{}p(t)\\delta_i(s,t)\n",
    "$$\n",
    "\n",
    "* Important aspects:\n",
    "    1. Does not address multicolinearity, the importance of the features will be diluted over all correlated features\n",
    "    2. Cannot be generalized to other non-tree based methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Mean drecrease accuracy\n",
    "* Out-of-sample method\n",
    "* The procedure to calculate the MDA is as follows:\n",
    "    1. Fits a classifier\n",
    "    2. Calculate the OOS score according to some metric\n",
    "    3. Permutate each column of $X$ and derive the OOS score\n",
    "    4. The importance of a feature according to the MDA method will be the mean loss of performance caused by its exlcusion across the permutations\n",
    "* Important aspects:\n",
    "    1. Can be applied to any classifier\n",
    "    2. Does not address multicolinearity, the importance of all correlated features will be diluted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Single feature importance\n",
    "* Cross-section out-of-sample method\n",
    "* It consists of computing the OOS predictive score for each feature in isolation \n",
    "* Important aspects:\n",
    "    1. Robust to multicolinearity\n",
    "    2. Can be applied to ny score function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Causality aware search procedures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Granger causality\n",
    "\n",
    "* Applied in [Huang et al. (2019)](https://link.springer.com/article/10.1007/s00181-019-01725-1)\n",
    "* **Assumptions [Eichler (2013)](https://royalsocietypublishing.org/doi/10.1098/rsta.2011.0613):**\n",
    "    >1. the effects does not precede its cause;\n",
    "    >2. the causal series contains unique information about the series being caused that is not available otherwise.\n",
    "    \n",
    "> **Definition [Eichler (2013)](https://royalsocietypublishing.org/doi/10.1098/rsta.2011.0613):** Let $V=(X, Y, Z)$ be a set of stochastic processes, where $X^t$, $Y^t$, and $Z^t$ represent all the information for each series up to time $t$. The series $X$ is Granger non-causal for the series $Y$ with respect to $V$ if $Y_{t+1} \\perp \\!\\!\\! \\perp X^t|Y^t,Z^t$, for all $t \\in \\mathbb{Z}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Dynamic Granger causality + Kaplan-Meier estimator\n",
    "* Applied in [Huang et al. (2019)](https://link.springer.com/article/10.1007/s00181-019-01725-1)\n",
    "* Same **assumptions** as above for the Granger causality method\n",
    "* Moving window and backtesting scheme\n",
    "\n",
    "> **Definition (dynamic Granger causality):** Let $V=(X, Y, Z)$ be a set of stochastic processes, where $X^{t:t+k}$, $Y^{t:t+k}$, and $Z^{t:t+k}$ represent all the information for each series from time $t$ to $t+k$, where $k$ is a moving window parameter. The series $X$ is Granger non-causal for the series $Y$ with respect to $V$ if $Y_{t+1} \\perp \\!\\!\\! \\perp X^{t:t+k}|Y^{t:t+k},Z^{t:t+k}$, for all $t,k \\in \\mathbb{Z}$ \n",
    "\n",
    "> **Definition (survival function) [Kleinbaum and Klein (2012)](https://www.springer.com/gp/book/9781441966452):** Let $T$ be  continuos random variable with cummulative distribution funcction given by $F(t)$ defined over $[0,\\infty]$. The survival function is given by:\n",
    "$$\n",
    "S(t)=P(T>t)=\\int_{t}^{\\infty}f(u)du=1-F(t)\n",
    "$$\n",
    "In our case, the measure $P(T>t)$ can be regarded as the probability that the persistence of the granger causality will survive beyond a value $t$ of time.\n",
    "\n",
    ">**Informal definition (Kaplan-Meier estimator):** The Kaplan-Meier estimator is a non-parametric statistic used to estimate the survival function from lifetime data. This estimator is given by:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{S}(t)=\n",
    "\\begin{cases}\n",
    "  1, & \\text{if}\\ t < t_1 \\\\\n",
    "  \\prod_{t_i \\leq t}^{} \\left(1-\\frac{d_i}{n_i}\\right), & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "where $t_i$ is the time where at least one event happened, $d_i$ is the number of events (i.e. deaths) that happened at time $t_i$, and $n_i$ is the number of inviduals known to have survived up to time $t_i$.\n",
    "\n",
    "* Huang et al. (2019) uses the Kaplan–Meier estimate to gauge ‘survival’ of a term’s Granger causality, and the persistence in its statistical significance.\n",
    "* They use a dummy variable to indicate when the series encounters a change in significance level, such that all series begin with an assigned value of 1 (irregardless of significance), and are assgined with 0 when the term either becomes Granger causal at any given point in time, or becomes no longer significantly Granger causal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Eichler causal identification [Eichler (2009)](https://www.researchgate.net/publication/228932592_Causal_inference_from_time_series_What_can_be_learned_from_granger_causality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 VAR-LiNGAM [Hyvarinem et al (2010)](https://www.cs.helsinki.fi/u/ahyvarin/papers/JMLR10.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
