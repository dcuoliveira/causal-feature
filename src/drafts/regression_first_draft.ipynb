{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trends: data exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import r2_score\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from vol4life.vol4life.plot import plot_acf, plot_ccf\n",
    "from vol4life.vol4life.stats import autocorrelation_f\n",
    "from word_list.basic import politics1, politics2\n",
    "from word_list.basic import business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create trends df from daily information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_path = os.path.join('data','daily_trend',\"*.csv\")\n",
    "daily_dfs_path =  glob(trends_path)\n",
    "daily_dfs_path.sort()\n",
    "daily_dfs = [pd.read_csv(path) for path in daily_dfs_path]\n",
    "daily_dfs_names = [i.split(\"/\")[2] for i in daily_dfs_path]\n",
    "daily_dfs_names = [i.split(\".\")[0] for i in daily_dfs_names]\n",
    "\n",
    "trends_df = []\n",
    "for name, df in zip(daily_dfs_names, daily_dfs):\n",
    "    df.index = pd.to_datetime(df.date)\n",
    "    ts = df[name]\n",
    "    new_name = name.replace(\" \", \"_\")\n",
    "    ts.name = new_name\n",
    "    trends_df.append(ts)\n",
    "\n",
    "final_date = \"2020-07-25\"\n",
    "trends_df = pd.concat(trends_df,1)\n",
    "trends_df = trends_df[:final_date]\n",
    "trends_df = trends_df.fillna(0.0)\n",
    "word_features = list(trends_df.columns)\n",
    "word_features.sort()\n",
    "trends_df = trends_df[word_features]\n",
    "trends_df_train = trends_df[:\"2010\"]\n",
    "trends_df_test = trends_df[\"2010\":]\n",
    "display(Markdown(\"### Google trends data\"))\n",
    "display(HTML(trends_df_train.head(5).to_html()))\n",
    "display(Markdown(\"#### data shape = {}\".format(trends_df_train.shape))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_df_p = trends_df_train.copy()\n",
    "trends_df_p.columns = [\"\"]* trends_df_p.shape[1]\n",
    "\n",
    "politics1_ts = trends_df_train[politics1].mean(1)\n",
    "politics1_ts.name = \"politics1\"\n",
    "politics2_ts = trends_df_train[politics2].mean(1)\n",
    "politics2_ts.name = \"politics2\"\n",
    "business_ts = trends_df_train[business].mean(1)\n",
    "business_ts.name = \"business\"\n",
    "\n",
    "display(Markdown(\"### Word Signal\"))\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "trends_df_p.plot(ax=ax,legend=False,alpha= 0.05,  color=\"mistyrose\");\n",
    "trends_df_train.mean(1).plot(ax=ax,legend=False, color=\"k\", label=\"mean signal\");\n",
    "ax.set_title(\"All Signals\");\n",
    "ax.legend(loc=\"best\");\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "politics1_ts.plot(ax=ax);\n",
    "politics2_ts.plot(ax=ax);\n",
    "business_ts.plot(ax=ax);\n",
    "ax.set_title(\"Word Signal by Category\");\n",
    "ax.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Trends Original Data Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = trends_df_train[politics1].corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "ax.set_title(\"Politics1 correlation\", fontsize=18)\n",
    "sns.heatmap(corr, mask=mask, cmap=\"coolwarm\", center=0, linewidths=0.5, annot=True, fmt=\".1f\", ax=ax);\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "\n",
    "corr = trends_df_train[politics2].corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "ax.set_title(\"Politics2 correlation\", fontsize=18)\n",
    "sns.heatmap(corr, mask=mask, cmap=\"coolwarm\", center=0, linewidths=0.5, annot=True, fmt=\".1f\", ax=ax);\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "\n",
    "corr = trends_df_train[business].corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "ax.set_title(\"Bussines correlation\", fontsize=18)\n",
    "sns.heatmap(corr, mask=mask, cmap=\"coolwarm\", center=0, linewidths=0.5, annot=True, fmt=\".1f\", ax=ax);\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "non_related_words = [\"happy\", \"garden\", \"fun\",\n",
    "                     \"food\", \"fine\", \"color\",\n",
    "                     \"arts\", \"travel\", \"housing\",\n",
    "                     \"legal\", \"leverage\", \"lifestyle\",\n",
    "                     \"BUY_AND_HOLD\", \"DOW_JONES\"]\n",
    "\n",
    "corr = trends_df_train[non_related_words].corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "ax.set_title(\"Non related words correlation\", fontsize=18)\n",
    "sns.heatmap(corr, mask=mask, cmap=\"coolwarm\", center=0, linewidths=0.5, annot=True, fmt=\".1f\", ax=ax);\n",
    "plt.xticks(rotation=45);\n",
    "\n",
    "corr = trends_df_train.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "corr_df = pd.DataFrame(corr.mask(cond=mask).values.flatten()).dropna()\n",
    "corr_df.columns = [\"correlation\"]\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "corr_df.boxplot(ax=ax, grid=False);\n",
    "ax.set_title(r\"Correlation distribution for all word pairs in the trends dataset\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_df.diff().head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "trends_std = scaler.fit_transform(trends_df_train)\n",
    "pca =  pca.fit(trends_std)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(pca.explained_variance_ratio_[:5])\n",
    "ax.set_xlabel('Principal component index', fontsize=14)\n",
    "ax.set_ylabel('Explained variance ratio', fontsize=14)\n",
    "ax.set_title(\"Selecting the number of PCA dimensions\", fontsize=18)\n",
    "\n",
    "trends_df_train_t = pd.DataFrame(pca.transform(trends_std)[:,:1],columns=[\"PCA1\"], index=trends_df_train.index)\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "trends_df_train_t.plot(ax=ax);\n",
    "ax.set_title(\"Transformed data\", fontsize=18);\n",
    "\n",
    "mean_norm = scaler.fit_transform(trends_df_train.mean(1).to_frame()).flatten()\n",
    "pca_norm = scaler.fit_transform(trends_df_train_t).flatten()\n",
    "trends_resume = pd.DataFrame(np.stack([pca_norm,mean_norm],1),columns=[\"PCA1_norm\", \"mean_norm\"], index=trends_df_train.index)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "trends_resume.plot(ax=ax);\n",
    "ax.set_title(\"Comparing PCA features and mean signal (both normalized)\", fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market data returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset = \"es1\"\n",
    "market_path = os.path.join('data','market',\"{}.txt\".format(asset))\n",
    "df_market = pd.read_csv(market_path, sep='\\t')\n",
    "close_price_column = 'PX_LAST'\n",
    "date_column='date'\n",
    "df_market.loc[:, date_column] = pd.to_datetime(df_market[date_column])\n",
    "df_market = df_market.set_index(date_column)\n",
    "close = df_market[[close_price_column]].dropna()\n",
    "close = close.pct_change().dropna()\n",
    "return_column = \"{}_returns\".format(asset)\n",
    "close.columns = [return_column]\n",
    "close.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging market data and trends data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge_asof(trends_df, close, left_index=True, right_index=True)\n",
    "merged_df = merged_df[[return_column] + word_features]\n",
    "display(Markdown(\"### Merged data\"))\n",
    "display(HTML(merged_df.head(5).to_html()))\n",
    "display(Markdown(\"#### data shape = {}\".format(merged_df.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring only one part of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = merged_df[:\"2010\"].copy()\n",
    "small_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = small_df.es1_returns\n",
    "trend = small_df.banking\n",
    "\n",
    "plot_acf(returns, lag_range=41, out_path=None, acf_function=autocorrelation_f)\n",
    "\n",
    "plot_acf(trend, lag_range=41, out_path=None, acf_function=autocorrelation_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_correlation = []\n",
    "lags = 30\n",
    "\n",
    "for c in word_features:\n",
    "    trend = small_df[c]\n",
    "    auto_correlation.append(np.mean(autocorrelation_f(trend,lags)[1:]))\n",
    "auto_correlation = pd.Series(auto_correlation, index=word_features).to_frame()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "auto_correlation.boxplot(ax=ax, grid=False);\n",
    "ax.set_title(\"Mean auto-correlation distribution for trends time series\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = small_df[word_features].corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "ax.set_title(\"All words correlation\", fontsize=18)\n",
    "sns.heatmap(corr, mask=mask, cmap=\"coolwarm\", center=0, linewidths=0.5, annot=False, fmt=\".2f\", ax=ax, cbar=True);\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Draft\n",
    "\n",
    "**We use the Time series cross validation from sklearn to observe the distribution\n",
    "of the statistics related to a simple linear regresion. The model is based only in one\n",
    "trend word. It tries to predict the next day return of the selected asset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only a part of the data\n",
    "\n",
    "small_df = merged_df[:\"2010\"].copy()\n",
    "small_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shifting returns. We use the word trend on day t\n",
    "# to predict returs on the day t+1\n",
    "\n",
    "small_df.loc[:, \"es1_returns\"] = small_df.es1_returns.shift(-1)\n",
    "small_df = small_df.dropna()\n",
    "small_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This functions uses the `TimeSeriesSplit` from sklearn to obtain different statistics based on a simple linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_ols_stats(df,return_column, select_word, n_splits):\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    is_scores = []\n",
    "    betas = []\n",
    "    t_stats = []\n",
    "    p_values = []\n",
    "    oos_scores = []\n",
    "    \n",
    "    for train_index, test_index in tscv.split(df):\n",
    "        df_train = df.iloc[train_index]\n",
    "        df_test = df.iloc[test_index]\n",
    "        formula = \"{} ~ {}\".format(return_column, select_word)\n",
    "        lr = smf.ols(formula=formula, data=df_train).fit()\n",
    "        is_scores.append(lr.rsquared)\n",
    "        betas.append(lr.params[1])\n",
    "        t_stats.append(lr.tvalues[1])\n",
    "        p_values.append(lr.pvalues[1])\n",
    "        y_pred = lr.predict(df_test).values\n",
    "        y_true = df_test[return_column]\n",
    "        oos_r2 = r2_score(y_true, y_pred)\n",
    "        oos_scores.append(oos_r2)\n",
    "    \n",
    "    dict_ = {\"IS_rsquared\":is_scores,\n",
    "             \"beta\":betas,\n",
    "             \"t-statistic\":t_stats,\n",
    "             \"p_value\":p_values,\n",
    "             \"OOS_rsquared\":oos_scores}\n",
    "        \n",
    "    return  pd.DataFrame(dict_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Results using some random words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "example =  word_features[4:20]\n",
    "stats = []\n",
    "p_values = []\n",
    "oos_scores = []\n",
    "is_scores = []\n",
    "\n",
    "## Getting the results for each word\n",
    "for word in example: \n",
    "    stat = get_simple_ols_stats(df=small_df,\n",
    "                                return_column=return_column,\n",
    "                                select_word=word,\n",
    "                                n_splits=n_splits)\n",
    "    stats.append(stat)\n",
    "\n",
    "## Combining results\n",
    "for word, stat in zip(example, stats):\n",
    "    ps = stat[\"p_value\"]\n",
    "    ps.name = word\n",
    "    p_values.append(ps)\n",
    "    is_score = stat[\"IS_rsquared\"]\n",
    "    is_score.name = word\n",
    "    is_scores.append(is_score)\n",
    "    os_score = stat[\"OOS_rsquared\"]\n",
    "    os_score.name = word\n",
    "    oos_scores.append(os_score)\n",
    "\n",
    "    \n",
    "## Plotting\n",
    "display(Markdown(\"### Simple Linear Model Results\"))\n",
    "display(Markdown(\"\"))\n",
    "\n",
    "p_values = pd.concat(p_values, 1)\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "p_values.boxplot(ax=ax, grid=False);\n",
    "ax.set_xticklabels(example, rotation=45);\n",
    "ax.set_title(\"P-value distribution for the coeficients of the explanatory variable\");\n",
    "\n",
    "is_scores = pd.concat(is_scores, 1)\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "is_scores.boxplot(ax=ax, grid=False);\n",
    "ax.set_xticklabels(example, rotation=45);\n",
    "ax.set_title(r\"In-sample $R^2$ distribution for each simple linear model\");\n",
    "\n",
    "\n",
    "oos_scores = pd.concat(oos_scores, 1)\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "oos_scores.boxplot(ax=ax, grid=False);\n",
    "ax.set_xticklabels(example, rotation=45);\n",
    "ax.set_title(r\"Out-of-sample $R^2$ distribution for each simple linear model\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_table  = oos_scores.mean(0).sort_values(ascending=False).to_frame().transpose()\n",
    "score_table.index = [r\"$R^2$\"]\n",
    "\n",
    "display(Markdown(\"### Out-of-sample mean $R^2$ for each model\"))\n",
    "display(HTML(score_table.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
