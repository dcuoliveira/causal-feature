{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('dcuoliveira': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d9012264f0201afc42ae5accaaf45fe1f2b8a1229337f3dc3b94fe4ebec69e8c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_mani.utils import merge_market_and_gtrends\n",
    "from prediction.functions import add_shift, get_features_granger_huang, new_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_params_search(df,\n",
    "                        wrapper,\n",
    "                        n_iter,\n",
    "                        n_splits,\n",
    "                        n_jobs,\n",
    "                        verbose,\n",
    "                        target_name=\"target_return\"):\n",
    "    \"\"\"\n",
    "    Use the dataframe 'df' to search for the best\n",
    "    params for the model 'wrapper'.\n",
    "\n",
    "    The CV split is performed using the TimeSeriesSplit\n",
    "    class.\n",
    "\n",
    "    We can define the size of the test set using the formula\n",
    "\n",
    "    ``n_samples//(n_splits + 1)``,\n",
    "\n",
    "\n",
    "    where ``n_samples`` is the number of samples. Hence,\n",
    "    we can define\n",
    "\n",
    "    n_splits = (n - test_size) // test_size\n",
    "\n",
    "\n",
    "    :param df: train data\n",
    "    :type df: pd.DataFrame\n",
    "    :param wrapper: predictive model\n",
    "    :type wrapper: sklearn model wrapper\n",
    "    :param n_iter: number of hyperparameter searchs\n",
    "    :type n_iter: int\n",
    "    :param n_splits: number of splits for the cross-validation\n",
    "    :type n_splits: int\n",
    "    :param n_jobs: number of concurrent workers\n",
    "    :type n_jobs: int\n",
    "    :param verbose: param to print iteration status\n",
    "    :type verbose: bool, int\n",
    "    :param target_name: name of the target column in 'df'\n",
    "    :type target_name: str\n",
    "    :return: R2 value\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "\n",
    "    X = df.drop(target_name, 1).values\n",
    "    y = df[target_name].values\n",
    "\n",
    "    time_split = TimeSeriesSplit(n_splits=n_splits)\n",
    "    r2_scorer = make_scorer(new_r2)\n",
    "\n",
    "    if wrapper.search_type == 'random':\n",
    "        model_search = RandomizedSearchCV(estimator=wrapper.ModelClass,\n",
    "                                          param_distributions=wrapper.param_grid,\n",
    "                                          n_iter=n_iter,\n",
    "                                          cv=time_split,\n",
    "                                          verbose=verbose,\n",
    "                                          n_jobs=n_jobs,\n",
    "                                          scoring=r2_scorer)\n",
    "    else:\n",
    "        model_search = RandomizedSearchCV(estimator=wrapper.ModelClass,\n",
    "                                          param_distributions=wrapper.param_grid,\n",
    "                                          n_iter=n_iter,\n",
    "                                          cv=time_split,\n",
    "                                          verbose=verbose,\n",
    "                                          n_jobs=n_jobs,\n",
    "                                          scoring=r2_scorer)\n",
    "\n",
    "    model_search = model_search.fit(X, y)\n",
    "\n",
    "    return model_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoWrapper():\n",
    "    def __init__(self, model_params=None):\n",
    "        self.model_name = \"lasso\"\n",
    "        self.search_type = 'random'\n",
    "        self.param_grid = {'alphas': np.linspace(0, 1, 100, endpoint=True)}\n",
    "        if model_params is None:\n",
    "            self.ModelClass = Lasso()\n",
    "        else:\n",
    "            self.ModelClass = Lasso(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_name = 'SPX Index'\n",
    "target_name=\"target_return\"\n",
    "max_lag = 20\n",
    "n_splits = 5\n",
    "verbose = False\n",
    "wrapper = LassoWrapper()\n",
    "path_list = [\"data\", \"gtrends.csv\"]\n",
    "\n",
    "ticker_path = \"data/indices/{}.csv\".format(ticker_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = merge_market_and_gtrends(ticker_path, test_size=0.5, path_gt_list=path_list)\n",
    "words = train.drop(target_name, 1).columns.to_list()\n",
    "complete = pd.concat([train, test])\n",
    "\n",
    "del train, test\n",
    "\n",
    "add_shift(merged_df=complete,\n",
    "            words=words,\n",
    "            max_lag=max_lag,\n",
    "            verbose=verbose)\n",
    "complete = complete.fillna(0.0)\n",
    "all_features = complete.drop(words + [target_name], 1).columns.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = get_features_granger_huang(ticker_name=ticker_name,\n",
    "                                    out_folder=\"indices\",\n",
    "                                    fs_method='granger',\n",
    "                                    path_list=path_list)\n",
    "\n",
    "complete_selected = complete[[target_name] + select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = complete_selected\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "years = df.index.map(lambda x: x.year)\n",
    "years = range(np.min(years), np.max(years))\n",
    "y = years[0]\n",
    "\n",
    "train_ys = df.loc[:str(y)]\n",
    "test_ys = df.loc[str(y + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Parameter grid for parameter (fit_intercept) needs to be a list or numpy array, but got (<class 'bool'>). Single values need to be wrapped in a list with one element.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ceed91dc704f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mr2_scorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_r2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m model_search = GridSearchCV(estimator=wrapper.ModelClass,\n\u001b[0m\u001b[1;32m      8\u001b[0m                             \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                             \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/dcuoliveira/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/dcuoliveira/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, estimator, param_grid, scoring, n_jobs, refit, cv, verbose, pre_dispatch, error_score, return_train_score)\u001b[0m\n\u001b[1;32m   1282\u001b[0m             return_train_score=return_train_score)\n\u001b[1;32m   1283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m         \u001b[0m_check_param_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/dcuoliveira/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_check_param_grid\u001b[0;34m(param_grid)\u001b[0m\n\u001b[1;32m    398\u001b[0m             if (isinstance(v, str) or\n\u001b[1;32m    399\u001b[0m                     not isinstance(v, (np.ndarray, Sequence))):\n\u001b[0;32m--> 400\u001b[0;31m                 raise ValueError(\"Parameter grid for parameter ({0}) needs to\"\n\u001b[0m\u001b[1;32m    401\u001b[0m                                  \u001b[0;34m\" be a list or numpy array, but got ({1}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                                  \u001b[0;34m\" Single values need to be wrapped in a list\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Parameter grid for parameter (fit_intercept) needs to be a list or numpy array, but got (<class 'bool'>). Single values need to be wrapped in a list with one element."
     ]
    }
   ],
   "source": [
    "X = df.drop(target_name, 1).values\n",
    "y = df[target_name].values\n",
    "\n",
    "time_split = TimeSeriesSplit(n_splits=n_splits)\n",
    "r2_scorer = make_scorer(new_r2)\n",
    "\n",
    "model_search = GridSearchCV(estimator=wrapper.ModelClass,\n",
    "                            param_grid=wrapper.param_grid,\n",
    "                            cv=time_split,\n",
    "                            verbose=verbose,\n",
    "                            scoring=r2_scorer)\n",
    "\n",
    "model_search = model_search.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = complete_selected\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "years = df.index.map(lambda x: x.year)\n",
    "years = range(np.min(years), np.max(years))\n",
    "for y in tqdm(years,\n",
    "              disable=not verbose,\n",
    "              desc=\"anual training and prediction\"):\n",
    "    train_ys = df.loc[:str(y)]\n",
    "    test_ys = df.loc[str(y + 1)]\n",
    "\n",
    "    # we have some roles in the time interval\n",
    "    # for some tickers, for example,\n",
    "    # \"SBUX UA Equity\"\n",
    "    if test_ys.shape[0] > 0:\n",
    "        model_wrapper = Wrapper()\n",
    "        model_search = hyper_params_search(df=train_ys,\n",
    "                                            wrapper=model_wrapper,\n",
    "                                            n_jobs=n_jobs,\n",
    "                                            n_splits=n_splits,\n",
    "                                            n_iter=n_iter,\n",
    "                                            verbose=verbose)\n",
    "        X_test = test_ys.drop(target_name, 1).values\n",
    "        y_test = test_ys[target_name].values\n",
    "        test_pred = model_search.best_estimator_.predict(X_test)\n",
    "        dict_ = {\"date\": test_ys.index,\n",
    "                    \"return\": y_test,\n",
    "                    \"prediction\": test_pred}\n",
    "        result = pd.DataFrame(dict_)\n",
    "        all_preds.append(result)\n",
    "    else:\n",
    "        pass"
   ]
  }
 ]
}